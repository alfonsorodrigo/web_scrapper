import argparse
import logging

logging.basicConfig(level=logging.INFO)

import re

from requests.exceptions import HTTPError
from urllib3.exceptions import MaxRetryError

from settings import SITES
from news_page_objects import HomePage, ArticlePage

is_well_formed_url = re.compile(
    r"^https?://.+/.+$"
)  # i.e. https://www.somesite.com/something
is_root_path = re.compile(r"^/.+$")  # i.e. /some-text

logger = logging.getLogger(__name__)


def _news_scraper(news_site_uid):
    host = SITES[news_site_uid]["url"]

    logging.info("Beginning scraper for {}".format(host))
    logging.info("Finding links in homepage...")

    homepage = HomePage(news_site_uid, host)

    articles = []

    for link in homepage.article_links:
        article = _fetch_article(news_site_uid, host, link)

        if article:
            logger.info("Article fetched!")
            articles.append(article)
            print(article.title)
            print(article.body)

    print(len(articles))


def _fetch_article(news_site_uid, host, link):
    logger.info("Start fetching article at {}".format(link))

    article = None
    try:
        article = ArticlePage(news_site_uid, _build_link(host, link))
    except (HTTPError, MaxRetryError) as e:
        logger.warn("Error while fetching article!", exc_info=False)

    if article and not article.body:
        logger.warn("Invalid article. There is no body.")
        return None

    return article


def _build_link(host, link):
    if is_well_formed_url.match(link):
        return link
    elif is_root_path.match(link):
        return "{host}{uri}".format(host=host, uri=link)
    else:
        return "{host}/{uri}".format(host=host, uri=link)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    news_site_choices = list(SITES.keys())
    parser.add_argument(
        "news_site",
        help="The news site that you want to scrape",
        type=str,
        choices=news_site_choices,
    )

    args = parser.parse_args()
    _news_scraper(args.news_site)
